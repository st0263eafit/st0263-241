{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "# WORDCOUNT COMPACTO\n",
        "# en AWS EMR:\n",
        "#files_rdd = sc.textFile(\"s3://st0263datasets/gutenberg-small/*.txt\")\n",
        "# en Google Cloud Platform:\n",
        "files_rdd = sc.textFile(\"gs://st0263datasets/gutenberg-small/*.txt\")\n",
        "# en HDFS:\n",
        "#files_rdd = sc.textFile(\"hdfs:///user/hadoop/datasets/gutenberg-small/*.txt\")\n",
        "wc_unsort = files_rdd.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "wc = wc_unsort.sortBy(lambda a: -a[1])\n",
        "for tupla in wc.take(10):\n",
        "        print(tupla)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "# WORDCOUNT PASO A PASO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "files_rdd = sc.textFile(\"gs://st0263datasets/gutenberg-small/*.txt\")\n",
        "#files = sc.textFile(\"hdfs:///user/hadoop/datasets/gutenberg-small/*.txt\")\n",
        "for f in files_rdd.take(10):\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "tokens = files_rdd.flatMap(lambda line: line.split())\n",
        "for t in tokens.take(10):\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "wc1 = tokens.map(lambda word: (word, 1))\n",
        "for c in wc1.take(10):\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "wc = wc1.reduceByKey(lambda a, b: a + b)\n",
        "for c in wc.take(10):\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "wcsort = wc.sortBy(lambda a: -a[1])\n",
        "for c in wcsort.take(10):\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "#salvar los datos de salida, fijarse que no exista: hdfs:///tmp/<your-username>wcout10\n",
        "wcsort.coalesce(1).saveAsTextFile(\"hdfs:///tmp/wcout3\")\n",
        "#si esta trabajando en aws (igual verifique que no exista previamente wcout10):\n",
        "#wcsort.coalesce(1).saveAsTextFile(\"s3://<your-public-bucket>/wcout2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%sh\n",
        "hdfs dfs -ls /tmp\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%sh\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "wc"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
